{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "1. Logarithmic Transformation\n",
    "2. Standardization and Scaling\n",
    "3. Polynomial Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logarithmic Transformation\n",
    "One of the principal tenets of the linear regression model is the idea that the relationship between the variables at play is linear. \n",
    "\n",
    "In cases when that is not necessarily true, we can apply transformation to the data that result in having a linear relationship. \n",
    "\n",
    "Once the linear model is obtained, we can then undo the transformation to obtain our final model.\n",
    "\n",
    "A typical transformation that is often used is applying a **logarithm** to *either one* or *both* of the dependent and response variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # pandas is a module for data reading\n",
    "\n",
    "mammals = pd.read_csv('./mammals.csv')     # csv is a common format for data storage\n",
    "\n",
    "print(type(mammals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "body_data = mammals['body']\n",
    "brain_data = mammals['brain']\n",
    "\n",
    "plt.scatter(body_data, brain_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "log_body_data = np.log(body_data)\n",
    "log_brain_data = np.log(brain_data)\n",
    "\n",
    "plt.scatter(log_body_data, log_brain_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation has helped us convert our problem into a simpler one. In this case, the relationship we see in this data may be modelled as a power law, i.e., $y=x^b$. With some middle school math:\n",
    "$$\\begin{aligned}\n",
    "\\log(y)&=\\log(x^b) \\\\ \n",
    "\\log(y)&=b \\log(x) \\\\ \n",
    "\\bar{y} &=b \\bar{x} \n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "This plot is called a *log-log plot*. There is also *semi-log plot*. Read after class. \n",
    "\n",
    "We can bulid a linear model from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "beta_1, beta_0, r, p, std_err = stats.linregress(log_body_data, log_brain_data)\n",
    "\n",
    "def myfunc(x):\n",
    "  return beta_0 + beta_1 * x\n",
    "\n",
    "mymodel = list(map(myfunc, log_body_data))\n",
    "\n",
    "plt.scatter(log_body_data, log_brain_data)\n",
    "plt.plot(log_body_data, mymodel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean on the original data? \n",
    "\n",
    "$$ \\begin{aligned} \n",
    "\\log(Brain) &= \\beta_0 + \\beta_1 \\log(Body) \\\\\n",
    "Brain &= e^{\\beta_0 + \\beta_1 \\log(Body)}\\\\\n",
    "Brain &= e^{\\beta_0} e^{\\beta_1 \\log(Body)}\\\\\n",
    "Brain &= e^{\\beta_0} {Body}^{\\beta_1}\n",
    "\\end{aligned}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc_powered(x):\n",
    "    return np.exp(beta_0) * (x**beta_1)\n",
    "\n",
    "mymodel_powered = list(map(myfunc_powered, body_data))\n",
    "\n",
    "plt.plot(body_data, mymodel_powered)\n",
    "\n",
    "plt.scatter(body_data, brain_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second, why does look so weird?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc_powered(x):\n",
    "    return np.exp(beta_0) * (x**beta_1)\n",
    "\n",
    "mymodel_powered = list(map(myfunc_powered, range(7000)))\n",
    "\n",
    "plt.plot(range(7000), mymodel_powered)\n",
    "\n",
    "plt.scatter(body_data, brain_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardization and Scaling\n",
    "\n",
    "there are many more tricks to pre-process the data in order to facilitate our modelling.\n",
    "\n",
    "One of those techniques consists on centring the independent variables such that their mean is zero. \n",
    "\n",
    "Another useful transformation is the scaling of our variables. This is convenient in cases where we have features that have very different scales, where some variables have large values and others have very small ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization or Unit Scaling\n",
    "The aim of this transformation is to convert the range of a given variable into a scale that goes from 0 to 1.\n",
    "\n",
    "$$f_{scaled}=\\frac{f-f_{min}}{f_{max}-f_{min}}$$\n",
    "where $f_{min}$ and $f_{max}$ are the minimal and maximal values of this feature in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mammals[['body', 'brain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaler.fit_transform(mammals[['body', 'brain']])  # Scaling applied to each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### z-Score Scaling\n",
    "An alternative method for scaling our features consists of taking into account how far away data points are from the mean.\n",
    "$$f_{z-score}=\\frac{f-\\mu_f}{\\sigma_f}$$\n",
    "where $\\mu_f$ is the mean and $\\sigma_f$ is the standard deviation of this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = preprocessing.StandardScaler()\n",
    "\n",
    "scaler2.fit_transform(mammals[['body','brain']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Polynomial Regression\n",
    "In the previous section we have seen how a simple transformation in the input and output variables make a complex model into a simpler one. In fact, we can try fitting different models using more and more complex functions. \n",
    "\n",
    "One important point to note is that a model is said to be linear when it is linear in the **parameters**. \n",
    "\n",
    "With that in mind, the 1-variable model\n",
    "$$ y=\\beta_0+\\beta_1 x + \\beta_2 x^2 +\\varepsilon$$\n",
    "and multivariate model \n",
    "$$ y=\\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 +\\beta_{11} x_1^2 + \\beta_{22} x_2^2 + \\beta_{12}x_1x_2+\\varepsilon$$\n",
    "are both linear as the parameters $\\beta_{i}$ are linear.\n",
    "\n",
    "In the examples above, the models are given by second order polynomials in one and two variables. When using such models to fit our data, they are called *polynomial regression* and in general the $k$-th order polynomial model in one variable is given by\n",
    "$$ y=\\beta_0+\\beta_1 x + \\beta_2 x^2 + ... + \\beta_k x^k+\\varepsilon.$$\n",
    "* Linear regression is first order polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial models can be very useful in cases where we know that nonlinear effects are present in the target variable.\n",
    "\n",
    "The polynomial model is effectively the [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series) of an unknown function and thus can be used to approximate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]\n",
    "y = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(range(25), mymodel(range(25)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.polyfit(x, y, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared ($R^2$) Score\n",
    "It is important to know how well the relationship between the values of the x- and y-axis is, if there are no relationship the polynomial regression can not be used to predict anything.\n",
    "\n",
    "The relationship is measured with a value called the R-squared.\n",
    "\n",
    "The r-squared value ranges from 0 to 1, where 0 means no relationship, and 1 means 100% related.\n",
    "\n",
    "Python and the Sklearn module will compute this value for you, all you have to do is feed it with the x and y arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(r2_score(y, mymodel(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [89,43,36,36,95,10,66,34,38,20,26,29,48,64,6,5,36,66,72,40]\n",
    "y = [21,46,3,35,67,95,53,72,58,10,26,34,90,33,38,20,56,2,47,15]\n",
    "\n",
    "mymodel = numpy.poly1d(numpy.polyfit(x, y, 3))\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(range(95), mymodel(range(95)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y, mymodel(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
