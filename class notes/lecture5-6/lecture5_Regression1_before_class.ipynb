{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "1. Regression\n",
    "2. Linear Regression\n",
    "3. Ordinary Least Squares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression\n",
    "Regression is one of the most well-known and widely used machine learning tool.\n",
    "\n",
    "We will cover here some of its most important aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationships between Variables\n",
    "\n",
    "Consider a situation where you are interested to determine the association between two (or more) pieces of information. For examples:\n",
    "\n",
    "1. the relation of the height of a child compared to that of her parents\n",
    "2. the relation of ice cream sales and outdoor temperature\n",
    "3. the relation of animals' body size and their brain size\n",
    "\n",
    "We can collect data for these events and use it for constructing a **model** that enables us to explore the relationship between the variables in question\n",
    "\n",
    "Ultimately, our goal is to use our model to predict the outcome of the variable of interest given the values of the other variable(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "* $\\mathbf{y}$: We usually call the quantity of interest the **response** or **dependent** variable and denote it with the variable $\\mathbf{y}$.\n",
    "\n",
    "* $\\mathbf{x}$: The other quantities are called **predictors**, **regressors** or **independent** variables and denote them as $\\mathbf{x}$.\n",
    "\n",
    "we say that two quantities are *correlated* if there is relationship between the two variables, i.e., the value of one\n",
    "tells us something about the value of the other one.\n",
    "\n",
    "We use regression to discover this underlying relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we continue...\n",
    "1. Just because we measure a correlation between two variables, it does not mean that there is a causal relationship between them.\n",
    "\n",
    "* The fact that people use umbrellas when it rains does not mean that umbrellas cause rain to fall.\n",
    "\n",
    " \n",
    "\n",
    "2. we must be careful when considering relationships between variables as they may be related to a third, confounding, variable\n",
    "\n",
    "* As Summer approaches, the ice cream van is busy selling more ice cones. A similar trend has been noted for the murder rates, as the heat rises, the number of murders do too. (New York Times: Jun 18th, 2009)\n",
    "\n",
    "* More ice cream sales causes more murders?\n",
    "\n",
    "* Always be on the lookout for confounding variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression towards the mean\n",
    "is a statistical phenomenon and can be seen as a fact of life.\n",
    "\n",
    "* (Galton, F. 1886) Tall parents are likely to have a child that is taller than average. However, the child is likely to be less tall than the parents. Short parents are likely to have children taller than the parents, but still below the average. This is true for both animals and plants.\n",
    "\n",
    "* A sprinter that breaks the world record in an a race is expected to run to his/her average time in the next one.\n",
    "\n",
    "This phenomenon is the foundation of regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "As a first attempt to determining the dependence among the variables, the simplest thing we can do is check if the relationship follows a **straight line**.\n",
    "\n",
    "$$\\mathbf{y} = f(\\mathbf{x})+\\varepsilon$$\n",
    "* $f(\\cdot)$: a linear function\n",
    "* $\\varepsilon$: noise\n",
    "\n",
    "$$\\mathbf{y} = \\beta_0+ \\beta_1\\mathbf{x}+\\varepsilon$$\n",
    "* $\\beta_0,\\beta_1$: regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [5,7,8,7,2,2,9,4,11,12,9,6]\n",
    "y = [99,86,87,88,111,103,87,94,78,77,85,86]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "beta_1, beta_0, r, p, std_err = stats.linregress(x, y)\n",
    "\n",
    "def myfunc(x):\n",
    "  return beta_0 + beta_1 * x\n",
    "\n",
    "mymodel = list(map(myfunc, x))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel,'b:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the \"best\" line that fits these data points, but best in what sense? We will discuss later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r for Relationship\n",
    "It is important to know how the relationship between the values of the x-axis and the values of the y-axis is, if there are no relationship the linear regression can not be used to predict anything.\n",
    "\n",
    "This relationship - the coefficient of correlation - is called r value.\n",
    "\n",
    "The r value ranges from -1 to 1, where 0 means no relationship, and 1 (and -1) means 100% related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this model to predict $\\mathbf{y}$'s value when $\\mathbf{x}=6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = myfunc(6)\n",
    "\n",
    "print(response)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel)\n",
    "plt.plot(6, response, 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [89,43,36,36,95,10,66,34,38,20,26,29,48,64,6,5,36,66,72,40]\n",
    "y = [21,46,3,35,67,95,53,72,58,10,26,34,90,33,38,20,56,2,47,15]\n",
    "\n",
    "beta_1, beta_0, r, p, std_err = stats.linregress(x, y)\n",
    "\n",
    "def myfunc(x):\n",
    "  return beta_0 + beta_1 * x\n",
    "\n",
    "mymodel = list(map(myfunc, x))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, mymodel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about p-value? [Read](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/) after class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression\n",
    "We can extend the model to include many more variables. Let's say have\n",
    "\n",
    "* $\\mathbf{y}=[y_1,y_2,...,y_n]^T$\n",
    "* $\\mathbf{x}_j=[x_{1j},x_{2j},...,x_{nj}]^T, ~~ \\forall j=1,2,...,m$\n",
    "\n",
    "This is saying that we have multi-dimensional information in each of $m$ data points, and we are interested in predicting something more complicated than a single number. \n",
    "\n",
    "The multivariate linear regression model is written as:\n",
    "$$y_i = \\beta_0+ \\sum_{j=1}^{m}\\beta_j \\mathbf{x}_j+\\varepsilon_i, ~~~ \\forall i=1,2,...,n \\text{ and } j=1,2,...,m.$$\n",
    "\n",
    "Writing in the matrix and vector forms:\n",
    "$$\\mathbf{X}=\\left(\\begin{matrix} 1 & x_{11} & x_{12} & ... &x_{1m}\\\\ 1 & x_{21} & x_{22} & ... &x_{2m}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & x_{n1} & x_{n2} & ... &x_{nm}\\\\\\end{matrix}\\right), \\mathbf{\\beta}=\\left(\\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\\\beta_2 \\\\ \\vdots \\\\\\beta_m\\end{matrix}\\right)$$\n",
    "$$\\mathbf{y}=\\left(\\begin{matrix} y_1 \\\\y_2 \\\\ \\vdots \\\\y_n\\end{matrix}\\right), \\mathbf{\\varepsilon}=\\left(\\begin{matrix} \\varepsilon_1 \\\\\\varepsilon_2 \\\\ \\vdots \\\\\\varepsilon_n\\end{matrix}\\right)$$\n",
    "\n",
    "We end up with the following form for the regression model:\n",
    "$$\\mathbf{y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\varepsilon}$$\n",
    "\n",
    "Our task is therefore to find the regression coefficients in the vector $\\mathbf{\\beta}$.\n",
    "\n",
    "* By the way, the textbook is full of typos in this chapter..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are tasked with finding the regression coefficients $\\mathbf{\\beta}$ in the multivariate regression model.\n",
    "\n",
    "If we were able to craft a perfect linear model, the actual value of $\\mathbf{y}$ would match exactly the prediction\n",
    "$f(\\mathbf{x}_1, \\mathbf{x}_2, ...,\\mathbf{x}_m)$. This implies that the residuals $\\mathbf{\\varepsilon}=\\mathbf{0}$.\n",
    "\n",
    "In a more realistic scenario, we would find a good line of best fit to the data points by minimizing the error. One way to implement a suitable objective function for this purpose is to minimize the *sum of squared residuals (SSR)*:\n",
    "$$\\begin{aligned} SSR &=\\sum_{i=1}^{n}\\varepsilon_i^2=\\|\\mathbf{\\varepsilon}\\|_2^2 ~~~~~~~~~~~~~~~~\\|\\cdot\\|_2 \\text{ is called $\\ell_2$-norm, it equals the Euclidean Distance}\\\\ &=\\|\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta}\\|_2^2 \\\\ &=(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})^T(\\mathbf{y}-\\mathbf{X}\\mathbf{\\beta})~~~~~~~~~~~~~~~~~~~~~\\text{since }a^T a = a_1^2 + a_2^2 + ...\\\\&=\\mathbf{y}^T\\mathbf{y}-\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}-\\mathbf{y}^T\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\\end{aligned}$$\n",
    "\n",
    "* This *sum of squared residuals* is how we define the \"best\" line in Python code above.\n",
    "* $\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}$ and $\\mathbf{y}^T\\mathbf{X}\\mathbf{\\beta}$ are both scalars (single numbers). So $\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}=(\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y})^T=\\mathbf{y}^T\\mathbf{X}\\mathbf{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is $\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}$ a scalar?\n",
    "\n",
    "$1\\times (m+1)$  $(m+1) \\times n$ $n \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $SSR$, we we take its derivative with respect to $\\mathbf{\\beta}$: \n",
    "$$\\begin{aligned}\\frac{\\partial (SSR)}{\\partial \\mathbf{\\beta}} &= \\frac{\\partial}{\\partial \\mathbf{\\beta}}\\left( \\mathbf{y}^T\\mathbf{y}-2\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta} \\right) \\\\ &= -2\\mathbf{X}^T\\mathbf{y}+2\\mathbf{X}^T\\mathbf{X}\\mathbf{\\beta}\\end{aligned}$$\n",
    "and set it to $\\mathbf{0}$. This gives us: \n",
    "$$\\mathbf{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "This is a closed form solution to the Ordinary Least Squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, dot\n",
    "from scipy import linalg\n",
    "\n",
    "X = array([[1, 1], [1, 2], [1, 3], [1, 4]])\n",
    "Y = array([[1], [2], [3], [4]])\n",
    "temp1 = linalg.inv(dot(X.T, X))\n",
    "temp2 = dot(X.T, Y)\n",
    "beta = dot(temp1,temp2)\n",
    "\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Use the closed form formula $\\mathbf{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}$ to verify the results produced by *stats.linregress* in the previous seesion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
